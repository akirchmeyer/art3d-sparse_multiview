{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7c9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WORKING: need to fix forward (unconditional generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d21b4a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (multiview.py, line 175)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.conda/envs/diff2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3433\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 5\u001b[0;36m\n\u001b[0;31m    from multiview import *\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/art3d/art3d-multiviewdepthdiffusion/visualizations/../sparse_multiview/multiview.py:175\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.multiview_cross =\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sparse_multiview = f'{os.getcwd()}/../sparse_multiview'\n",
    "sys.path.append(sparse_multiview)\n",
    "from multiview import *\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "args = OmegaConf.load(f'{sparse_multiview}/configs/views5.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiViewDiffusionModel(args).cuda()\n",
    "model.eval()\n",
    "dataset = MultiViewDataset(args)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    #pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ca5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(batch['target'].shape, batch['source'].shape)\n",
    "target = batch['target'].squeeze(0).permute((1, 2, 0))\n",
    "source = batch['source'].squeeze(0).permute((1, 2, 0))\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "axs[0].imshow((target+1)/2)\n",
    "axs[0].imshow((source+1)/2)\n",
    "axs[1].imshow(batch['depth'].squeeze())\n",
    "axs[2].imshow(batch['mask'].squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(dataloader))\n",
    "# image = batch['image'].squeeze(0).permute((1, 2, 0))\n",
    "# views = [img.permute((1, 2, 0)) for img in batch['views'].squeeze(0)]\n",
    "# fig, axs = plt.subplots(1, 3)\n",
    "# axs[0].imshow((image+1)/2)\n",
    "# axs[1].imshow(batch['depth'].squeeze())\n",
    "# axs[2].imshow(batch['mask'].squeeze())\n",
    "# plt.show()\n",
    "# fig, axs = plt.subplots(1, len(views), figsize=(40,40))\n",
    "# for i, view in enumerate(views):\n",
    "#     axs[i].imshow(view)\n",
    "#     axs[i].set_axis_off()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba891c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82701e62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mforward({k:v\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m      2\u001b[0m images[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "images = model.forward({k:v.cuda() for k,v in batch.items()})\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.multiview_encoder.mae_image_mask_ratio = 0\n",
    "images, attention_maps = model.forward_with_crossattention({k:v.cuda() for k,v in batch.items()})\n",
    "attention_maps = attention_maps.detach().cpu()\n",
    "orig_image = images[0]\n",
    "orig_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.num_views_per_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by \n",
    "# https://github.com/AttendAndExcite/Attend-and-Excite/blob/main/notebooks/explain.ipynb\n",
    "# https://github.com/AttendAndExcite/Attend-and-Excite/blob/main/utils/vis_utils.py \n",
    "# https://github.com/AttendAndExcite/Attend-and-Excite/blob/main/utils/ptp_utils.py\n",
    "\n",
    "import ptp_utils\n",
    "from vis_utils import show_image_relevance\n",
    "# show spatial attention for indices of tokens to strengthen\n",
    "n_tokens = attention_maps.shape[2]\n",
    "res = 16\n",
    "\n",
    "mae = model.multiview_encoder.model\n",
    "m = n_tokens // dataset.num_views_per_inst\n",
    "\n",
    "context = batch['views']\n",
    "context = context.view(-1, *context.shape[2:])\n",
    "context_tokens = mae.patchify(context.cuda()).reshape(-1, 768).cpu().numpy()\n",
    "non_white = np.abs(context_tokens-1).mean(axis=1)\n",
    "context_tokens = np.array([context_tokens[i] for i in range(len(context_tokens)) if non_white[i] >= 1e-2])\n",
    "\n",
    "for view in range(dataset.num_views_per_inst):\n",
    "    images = []\n",
    "    for i in range(view*m, view*m+m):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = show_image_relevance(image, orig_image)\n",
    "        image = image.astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((res ** 2, res ** 2)))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images[1:11], axis=0))\n",
    "    plt.show()\n",
    "    \n",
    "    imgs = context_tokens.reshape(-1, 3, 16, 16).transpose((0,2,3,1))\n",
    "    #imgs = context_tokens[:10].reshape(-1, 16, 16, 3)\n",
    "    #print(imgs[0])\n",
    "    fig, axs = plt.subplots(1, len(imgs))\n",
    "    for i in range(len(imgs)):\n",
    "        axs[i].imshow(imgs[i])\n",
    "        axs[i].set_axis_off()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
